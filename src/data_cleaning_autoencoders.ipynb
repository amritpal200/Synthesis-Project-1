{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this file is to clean the data for the autoencoders model. Here, we can create a cleaned csv for training and another for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\arroc\\anaconda3\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from tokenizers) (0.23.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "%pip install tokenizers\n",
    "from skipgram import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a combined csv for all the logs except for the last file\n",
    "\n",
    "list_csv=[\"sitges_access.20240122\", \"sitges_access.20240123\", \"sitges_access.20240124\", \"sitges_access.20240125\", \"sitges_access.20240126\", \"sitges_access.20240127\"]\n",
    "\n",
    "dataframes = []\n",
    "for csv_file in list_csv:\n",
    "    file = f\"../data/{csv_file}.csv\"\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "combined_df.to_csv(\"../data/combined_csvs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2277/2277 [00:09<00:00, 242.59it/s]\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "\n",
    "# List with all possible csv to clean\n",
    "files=[\"combined_csvs\", \"sitges_access.20240129\"] # files[0] is for training and files[1] is for testing\n",
    "\n",
    "file = f\"../data/{files[0]}.csv\" # Change the file as needed \n",
    "\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# shuffle the rows\n",
    "df_clean = df_clean.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# remove columns\n",
    "df_clean = df_clean.drop(columns=[\"logname\", \"authenticate\", \"Unnamed: 0\", \"server_name\"])\n",
    "\n",
    "df_temp = df_clean.copy()\n",
    "groups = df_temp.groupby([\"IP\", \"user-agent\"])\n",
    "df_temp[\"group\"] = groups.ngroup()\n",
    "counts = df_temp.value_counts(\"group\")\n",
    "df_clean[\"group\"] = df_temp[\"group\"]\n",
    "\n",
    "df_temp = df_clean.copy()\n",
    "df_temp[\"date\"] = pd.to_datetime(df_temp[\"date\"])\n",
    "df_temp[\"date\"].head()\n",
    "\n",
    "# sort groups by date\n",
    "df_temp = df_temp.sort_values(\"date\")\n",
    "for i, g in tqdm(enumerate(df_temp[\"group\"].unique()), total=df_temp[\"group\"].nunique()):\n",
    "\tgroup = df_temp.loc[df_temp[\"group\"] == g].copy()\n",
    "\tgroup[\"elapsed\"] = group[\"date\"].diff().dt.total_seconds()\n",
    "\tgroup[\"elapsed\"] = group[\"elapsed\"].fillna(0)\n",
    "\tgroup[\"elapsed\"] = np.log(group[\"elapsed\"]+1)\n",
    "\tdf_temp.loc[df_temp[\"group\"] == g, \"elapsed\"] = group[\"elapsed\"]\n",
    "\t\n",
    "df_clean[\"elapsed\"] = df_temp[\"elapsed\"]\n",
    "df_clean[\"order\"] = df_temp.reset_index().index\n",
    "\n",
    "\n",
    "normalize_path = os.path.join(ROOT_DIR, \"models/normalize.json\")\n",
    "\n",
    "with open(normalize_path, \"r\") as f:\n",
    "\tnormalize = json.load(f)\n",
    "\tdf_temp = df_clean.copy()\n",
    "\tIP_octs = df_temp[\"IP\"].apply(lambda x: x.split(\".\"))\n",
    "\tfor i in range(4):\n",
    "\t\tdf_temp[\"IP_oct\" + str(i)] = IP_octs.apply(lambda x: float(x[i]))\n",
    "\t\tmean, std = df_temp[\"IP_oct\" + str(i)].mean(), df_temp[\"IP_oct\" + str(i)].std()\n",
    "\t\tdf_temp[\"IP_oct\" + str(i)] = (df_temp[\"IP_oct\" + str(i)] - mean) / std\n",
    "\t\tnormalize[\"IP_oct\" + str(i)] = {\"mean\": mean, \"std\": std}\n",
    "\tjson.dump(normalize, open(normalize_path, \"w\"))\n",
    "\n",
    "df_temp = df_temp.drop(columns=[\"IP\"])\n",
    "df_clean = df_temp.copy()\n",
    "\n",
    "df_clean[\"date\"] = df[\"date\"].str.split(\"+\").str[0]\n",
    "\n",
    "def sin_transform(x, period=24):\n",
    "\treturn np.sin(2 * np.pi * x / period)\n",
    "\n",
    "def cos_transform(x, period=24):\n",
    "\treturn np.cos(2 * np.pi * x / period)\n",
    "\n",
    "df_temp = df_clean.copy()\n",
    "\n",
    "df_temp[\"date\"] = pd.to_datetime(df_temp[\"date\"])\n",
    "df_temp[\"month\"] = df_temp[\"date\"].dt.month\n",
    "df_temp[\"day\"] = df_temp[\"date\"].dt.day\n",
    "df_temp[\"weekday\"] = df_temp[\"date\"].dt.weekday\n",
    "df_temp[\"hour\"] = df_temp[\"date\"].dt.hour\n",
    "df_temp[\"minute\"] = df_temp[\"date\"].dt.minute\n",
    "df_temp[\"month_sin\"] = sin_transform(df_temp[\"month\"], 12)\n",
    "df_temp[\"month_cos\"] = cos_transform(df_temp[\"month\"], 12)\n",
    "df_temp[\"day_sin\"] = sin_transform(df_temp[\"day\"], 31)\n",
    "df_temp[\"day_cos\"] = cos_transform(df_temp[\"day\"], 31)\n",
    "df_temp[\"weekday_sin\"] = sin_transform(df_temp[\"weekday\"], 7)\n",
    "df_temp[\"weekday_cos\"] = cos_transform(df_temp[\"weekday\"], 7)\n",
    "df_temp[\"hour_sin\"] = sin_transform(df_temp[\"hour\"], 24)\n",
    "df_temp[\"hour_cos\"] = cos_transform(df_temp[\"hour\"], 24)\n",
    "df_temp[\"minute_sin\"] = sin_transform(df_temp[\"minute\"], 60)\n",
    "df_temp[\"minute_cos\"] = cos_transform(df_temp[\"minute\"], 60)\n",
    "\n",
    "df_temp.drop([\"date\", 'month', 'day', 'weekday', 'hour', 'minute'], axis=1, inplace=True)\n",
    "\n",
    "df_clean = df_temp.copy()\n",
    "\n",
    "df_clean = pd.get_dummies(df_clean, columns=[\"petition\"], dtype=int)\n",
    "columns_to_combine = [\"petition_CONNECT\", \"petition_USER\", \"petition_OPTIONS\"] \n",
    "df_clean[\"petition_other\"] = df_clean[columns_to_combine].max(axis=1) # combine using OR\n",
    "df_clean = df_clean.drop(columns=columns_to_combine)\n",
    "\n",
    "# URL  \n",
    "df_temp = df_clean.copy()\n",
    "df_temp = df_temp[df_temp[\"URL\"].str.contains(r\"HTTP/\\d+\\.\\d+\").fillna(False)]\n",
    "\n",
    "embeddings_url = load_embeddings(os.path.join(ROOT_DIR, \"models/embeddings-url.pt\"))\n",
    "idx2word_url = load_idx2word(os.path.join(ROOT_DIR, \"models/idx2word-url.json\"))\n",
    "tokenizer_url = load_tokenizer(os.path.join(ROOT_DIR, \"models\"), \"charbpe-url\")\n",
    "embeddings_url.shape, embeddings_url.mean(), embeddings_url.std()\n",
    "\n",
    "df_clean = df_temp.copy()\n",
    "\n",
    "# Status\n",
    "df_temp = df_clean.copy()\n",
    "df_temp[\"status\"] = df_temp[\"status\"].apply(lambda x: str(x)[0])\n",
    "df_temp[\"status_1\"] = False\n",
    "df_temp = pd.get_dummies(df_temp, columns=[\"status\"], dtype=int)\n",
    "df_clean = df_temp.copy()\n",
    "\n",
    "# Bytes\n",
    "df_temp = df_clean.copy()\n",
    "df_temp[\"bytes\"] = np.log(df_temp[\"bytes\"]+1)\n",
    "\n",
    "mean, std = df_temp[\"bytes\"].mean(), df_temp[\"bytes\"].std()\n",
    "bytes_scaled = (df_temp[\"bytes\"] - mean) / std\n",
    "df_temp[\"bytes\"] = bytes_scaled\n",
    "df_clean = df_temp.copy()\n",
    "\n",
    "with open(normalize_path, \"r\") as f:\n",
    "\tnormalize = json.load(f)\n",
    "\tnormalize[\"bytes\"] = {\"mean\": mean, \"std\": std}\n",
    "\tjson.dump(normalize, open(normalize_path, \"w\"))\n",
    "\t\n",
    "# Referer\n",
    "embeddings_referer = load_embeddings(os.path.join(ROOT_DIR, \"models/embeddings-referer.pt\"))\n",
    "idx2word_referer = load_idx2word(os.path.join(ROOT_DIR, \"models/idx2word-referer.json\"))\n",
    "tokenizer_referer = load_tokenizer(os.path.join(ROOT_DIR, \"models\"), \"charbpe-referer\")\n",
    "\n",
    "df_temp = df_clean.copy()\n",
    "df_temp = df_temp.dropna(subset=[\"referer\"])\n",
    "\n",
    "df_clean = df_temp.copy()\n",
    "\n",
    "# User agent\n",
    "embeddings_useragent = load_embeddings(os.path.join(ROOT_DIR, \"models/embeddings-useragent.pt\"))\n",
    "idx2word_useragent = load_idx2word(os.path.join(ROOT_DIR, \"models/idx2word-useragent.json\"))\n",
    "tokenizer_useragent = load_tokenizer(os.path.join(ROOT_DIR, \"models\"), \"charbpe-useragent\")\n",
    "\n",
    "df_temp = df_clean.copy()\n",
    "df_temp = df_temp.dropna(subset=[\"user-agent\"])\n",
    "\n",
    "df_clean = df_temp.copy()\n",
    "\n",
    "\n",
    "# Save df_clean to csv\n",
    "df_clean = df_clean.sort_values(\"order\")\n",
    "df_clean = df_clean.drop(columns=[\"order\"])\n",
    "if file == f\"../data/{files[0]}.csv\":\n",
    "\tdf_clean.to_csv(os.path.join(ROOT_DIR, \"data/sitges_access_clean_whole_set_but_last.csv\"), index=False)\n",
    "else:\n",
    "\tdf_clean.to_csv(os.path.join(ROOT_DIR, \"data/sitges_access_clean_last.csv\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
