{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this file is to preprocess the data for the autoencoders model. Here, we can create a preprocessed csv for training and another for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from skipgram import *\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[\"sitges_access_clean_whole_set_but_last\", \"sitges_access_clean_last\"] # files[0] is for training and files[1] is for testing\n",
    "\n",
    "file=f'../data/{files[0]}.csv' # Change the file as needed\n",
    "\n",
    "logs_df = pd.read_csv(file)\n",
    "\n",
    "logs_df['status_1'] = logs_df['status_1'].fillna(False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/58365 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58365/58365 [00:10<00:00, 5454.44it/s]\n",
      "100%|██████████| 58365/58365 [00:11<00:00, 5046.36it/s]\n",
      "100%|██████████| 58365/58365 [00:11<00:00, 5266.18it/s]\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings_url = load_embeddings(os.path.join(ROOT_DIR, \"models\", \"embeddings-url.pt\"))\n",
    "# Load the idx2word. This is the vocabulary where each token is associated with an index\n",
    "idx2word_url = load_idx2word(os.path.join(ROOT_DIR, \"models\", \"idx2word-url.json\"))\n",
    "# Load the tokenizer. Just specify the name `charbpe-url` and it will load the tokenizer, which is saved\n",
    "# in the files `charbpe-url-vocab.json` and `charbpe-url-merges.txt`\n",
    "tokenizer_url = load_tokenizer(os.path.join(ROOT_DIR, \"models\"), \"charbpe-url\")\n",
    "\n",
    "url_embeddings = extract_embeddings(\n",
    "\tsequence = logs_df[\"URL\"],\n",
    "\tembeddings = embeddings_url,\n",
    "\tidx2word = idx2word_url,\n",
    "\ttokenizer = tokenizer_url\n",
    ")\n",
    "\n",
    "embeddings_referer = load_embeddings(os.path.join(ROOT_DIR, \"models/embeddings-referer.pt\"))\n",
    "idx2word_referer = load_idx2word(os.path.join(ROOT_DIR, \"models/idx2word-referer.json\"))\n",
    "tokenizer_referer = load_tokenizer(os.path.join(ROOT_DIR, \"models\"), \"charbpe-referer\")\n",
    "embeddings_referer.shape, embeddings_referer.mean(), embeddings_referer.std()\n",
    "\n",
    "# --- this will take additional 3.3 GB of memory---\n",
    "referers_embeddings = extract_embeddings(\n",
    "\tsequence = logs_df[\"referer\"],\n",
    "\tembeddings = embeddings_referer,\n",
    "\tidx2word = idx2word_referer,\n",
    "\ttokenizer = tokenizer_referer\n",
    ")\n",
    "\n",
    "embeddings_useragent = load_embeddings(os.path.join(ROOT_DIR, \"models/embeddings-useragent.pt\"))\n",
    "idx2word_useragent = load_idx2word(os.path.join(ROOT_DIR, \"models/idx2word-useragent.json\"))\n",
    "tokenizer_useragent = load_tokenizer(os.path.join(ROOT_DIR, \"models\"), \"charbpe-useragent\")\n",
    "embeddings_useragent.shape, embeddings_useragent.mean(), embeddings_useragent.std()\n",
    "\n",
    "# --- this will take additional 3.3 GB of memory---\n",
    "useragents_embeddings = extract_embeddings(\n",
    "\tsequence = logs_df[\"user-agent\"],\n",
    "\tembeddings = embeddings_useragent,\n",
    "\tidx2word = idx2word_useragent,\n",
    "\ttokenizer = tokenizer_useragent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]\n",
    "referers=[]\n",
    "usernames=[]\n",
    "\n",
    "for url, referer, username in zip(url_embeddings, referers_embeddings, useragents_embeddings):\n",
    "    urls.append(url.mean(0).float().numpy())\n",
    "    referers.append(referer.mean(0).float().numpy())\n",
    "    usernames.append(username.mean(0).float().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the columns are in the correct order\n",
    "logs_df = logs_df.reindex(columns=['bytes','elapsed', 'IP_oct0', 'IP_oct1', 'IP_oct2', 'IP_oct3', 'month_sin',\n",
    "       'month_cos', 'day_sin', 'day_cos', 'weekday_sin', 'weekday_cos',\n",
    "       'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'petition_-',\n",
    "       'petition_GET', 'petition_HEAD', 'petition_POST', 'petition_other',\n",
    "       'status_1', 'status_2', 'status_3', 'status_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['bytes', 'elapsed', 'IP_oct0', 'IP_oct1', 'IP_oct2', 'IP_oct3',\n",
      "       'month_sin', 'month_cos', 'day_sin', 'day_cos', 'weekday_sin',\n",
      "       'weekday_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos',\n",
      "       'petition_-', 'petition_GET', 'petition_HEAD', 'petition_POST',\n",
      "       'petition_other', 'status_1', 'status_2', 'status_3', 'status_4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def convert_to_sequence(list_sequence):\n",
    "    # Convert sequences to PyTorch tensors\n",
    "    sequences = [torch.tensor(seq, dtype=torch.float32) for seq in list_sequence]\n",
    "\n",
    "    # Padding sequences\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # Convert padded sequences back to DataFrame\n",
    "    padded_df = pd.DataFrame(padded_sequences.numpy())\n",
    "\n",
    "    return padded_df\n",
    "\n",
    "def add_sequence_to_dataframe(logs_df, df, column_after):\n",
    "    df1_part1 = logs_df.iloc[:, :logs_df.columns.get_loc(column_after)]\n",
    "    df1_part2 = logs_df.iloc[:, logs_df.columns.get_loc(column_after):]\n",
    "\n",
    "    # Concatenate the parts with df2 in between\n",
    "    logs_df = pd.concat([df1_part1, df, df1_part2], axis=1)\n",
    "    return logs_df\n",
    "\n",
    "\n",
    "\n",
    "sequenced_urls=convert_to_sequence(urls)\n",
    "sequenced_referers=convert_to_sequence(referers)\n",
    "sequenced_usernames=convert_to_sequence(usernames)\n",
    "print(logs_df.columns)\n",
    "# Add the sequences to the original dataframe\n",
    "logs_df=add_sequence_to_dataframe(logs_df, sequenced_urls, \"bytes\")\n",
    "\n",
    "logs_df=add_sequence_to_dataframe(logs_df, sequenced_referers, \"elapsed\")\n",
    "logs_df=add_sequence_to_dataframe(logs_df, sequenced_usernames, \"elapsed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if file==f'../data/{files[0]}.csv':\n",
    "    logs_df.to_csv(os.path.join(ROOT_DIR, \"data/sitges_access_prepared_whole_set_but_last.csv\"), index=False)\n",
    "else:\n",
    "    logs_df.to_csv(os.path.join(ROOT_DIR, \"data/sitges_access_prepared_last.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
